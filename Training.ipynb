{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python (rltrader_env)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys, os, time, json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from func_tools import import_px_data, standardize, cnn_data_reshaping, reshape_lob_levels, label_insights, back_to_labels, get_strategy_pnl,intraday_vol_ret \n",
    "import visualization_tools as viz_t\n",
    "from labelling_class import Labels_Generator\n",
    "\n",
    "import inspect\n",
    "\n",
    "import plotly_express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Reshape, Conv2D, LSTM, Dense, MaxPooling2D, BatchNormalization, LeakyReLU, concatenate, add, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict tf to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=6024)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "source": [
    "## Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other inputs\n",
    "length = 100\n",
    "batch_size = 64\n",
    "\n",
    "#trading_fee=0.000712"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading cached Experiments/cache/USDT_BTC/TRAIN--dyn_z_score-43200--USDT_BTC--10lev--10sec--2020_04_04--2021_01_03.csv.gz\n",
      "Reading cached Experiments/cache/USDT_BTC/TEST--dyn_z_score-43200--USDT_BTC--10lev--10sec--2020_04_04--2021_01_03.csv.gz\n",
      "Reading cached Experiments/cache/USDT_BTC/TRAIN_TOP--USDT_BTC--10lev--10sec--2020_04_04--2021_01_03.csv.gz\n",
      "Reading cached Experiments/cache/USDT_BTC/TEST_TOP--USDT_BTC--10lev--10sec--2020_04_04--2021_01_03.csv.gz\n"
     ]
    }
   ],
   "source": [
    "experiments_folder = 'Experiments'\n",
    "frequency = timedelta(seconds=10)\n",
    "pair = 'USDT_BTC'\n",
    "date_start = '2020_04_04'\n",
    "date_end = '2021_01_03'\n",
    "lob_depth = 10\n",
    "norm_type = 'dyn_z_score'\n",
    "roll = 7200 * 6\n",
    "\n",
    "# import data\n",
    "train_dyn_df, test_dyn_df, top_ob_train, top_ob_test = import_px_data(experiments_folder, frequency, pair, date_start, date_end, lob_depth, norm_type, roll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                  Datetime  Unnamed: 0.1     Ask_Price  Ask_Size  \\\n",
       "Level                                                              \n",
       "0      2020-04-08 00:00:10            10   7202.024099  0.067098   \n",
       "0      2020-04-08 00:00:20            20   7204.080836  0.067272   \n",
       "0      2020-04-08 00:00:30            30   7204.489362  0.067272   \n",
       "0      2020-04-08 00:00:40            40   7203.768000  0.067272   \n",
       "0      2020-04-08 00:00:50            50   7200.659216  0.067113   \n",
       "...                    ...           ...           ...       ...   \n",
       "0      2020-10-13 04:47:10         17230  11469.824091  2.808632   \n",
       "0      2020-10-13 04:47:20         17240  11469.824091  2.808632   \n",
       "0      2020-10-13 04:47:30         17250  11469.824091  2.808632   \n",
       "0      2020-10-13 04:47:40         17260  11469.824091  2.808632   \n",
       "0      2020-10-13 04:47:50         17270  11469.824091  2.808632   \n",
       "\n",
       "          Bid_Price  Bid_Size    Sequence     Mid_Price        Spread  \\\n",
       "Level                                                                   \n",
       "0       7200.299984  2.000000   725270075   7201.162041  2.394218e-04   \n",
       "0       7201.740462  2.000000   725270346   7202.910649  3.249207e-04   \n",
       "0       7202.820973  2.000000   725270702   7203.655168  2.316032e-04   \n",
       "0       7201.380325  0.031638   725270941   7202.574163  3.315030e-04   \n",
       "0       7199.052925  2.000000   725271192   7199.856070  2.231003e-04   \n",
       "...             ...       ...         ...           ...           ...   \n",
       "0      11469.824091  0.017224  1035445777  11469.824091  8.717644e-13   \n",
       "0      11469.824091  0.017224  1035445777  11469.824091  8.717644e-13   \n",
       "0      11469.824091  0.017224  1035445777  11469.824091  8.717644e-13   \n",
       "0      11469.824091  0.017224  1035445777  11469.824091  8.717644e-13   \n",
       "0      11469.824091  0.017224  1035445777  11469.824091  8.717644e-13   \n",
       "\n",
       "       merge_index  \n",
       "Level               \n",
       "0                0  \n",
       "0                1  \n",
       "0                2  \n",
       "0                3  \n",
       "0                4  \n",
       "...            ...  \n",
       "0          1626042  \n",
       "0          1626043  \n",
       "0          1626044  \n",
       "0          1626045  \n",
       "0          1626046  \n",
       "\n",
       "[1626047 rows x 10 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Datetime</th>\n      <th>Unnamed: 0.1</th>\n      <th>Ask_Price</th>\n      <th>Ask_Size</th>\n      <th>Bid_Price</th>\n      <th>Bid_Size</th>\n      <th>Sequence</th>\n      <th>Mid_Price</th>\n      <th>Spread</th>\n      <th>merge_index</th>\n    </tr>\n    <tr>\n      <th>Level</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2020-04-08 00:00:10</td>\n      <td>10</td>\n      <td>7202.024099</td>\n      <td>0.067098</td>\n      <td>7200.299984</td>\n      <td>2.000000</td>\n      <td>725270075</td>\n      <td>7201.162041</td>\n      <td>2.394218e-04</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2020-04-08 00:00:20</td>\n      <td>20</td>\n      <td>7204.080836</td>\n      <td>0.067272</td>\n      <td>7201.740462</td>\n      <td>2.000000</td>\n      <td>725270346</td>\n      <td>7202.910649</td>\n      <td>3.249207e-04</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2020-04-08 00:00:30</td>\n      <td>30</td>\n      <td>7204.489362</td>\n      <td>0.067272</td>\n      <td>7202.820973</td>\n      <td>2.000000</td>\n      <td>725270702</td>\n      <td>7203.655168</td>\n      <td>2.316032e-04</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2020-04-08 00:00:40</td>\n      <td>40</td>\n      <td>7203.768000</td>\n      <td>0.067272</td>\n      <td>7201.380325</td>\n      <td>0.031638</td>\n      <td>725270941</td>\n      <td>7202.574163</td>\n      <td>3.315030e-04</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2020-04-08 00:00:50</td>\n      <td>50</td>\n      <td>7200.659216</td>\n      <td>0.067113</td>\n      <td>7199.052925</td>\n      <td>2.000000</td>\n      <td>725271192</td>\n      <td>7199.856070</td>\n      <td>2.231003e-04</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2020-10-13 04:47:10</td>\n      <td>17230</td>\n      <td>11469.824091</td>\n      <td>2.808632</td>\n      <td>11469.824091</td>\n      <td>0.017224</td>\n      <td>1035445777</td>\n      <td>11469.824091</td>\n      <td>8.717644e-13</td>\n      <td>1626042</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2020-10-13 04:47:20</td>\n      <td>17240</td>\n      <td>11469.824091</td>\n      <td>2.808632</td>\n      <td>11469.824091</td>\n      <td>0.017224</td>\n      <td>1035445777</td>\n      <td>11469.824091</td>\n      <td>8.717644e-13</td>\n      <td>1626043</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2020-10-13 04:47:30</td>\n      <td>17250</td>\n      <td>11469.824091</td>\n      <td>2.808632</td>\n      <td>11469.824091</td>\n      <td>0.017224</td>\n      <td>1035445777</td>\n      <td>11469.824091</td>\n      <td>8.717644e-13</td>\n      <td>1626044</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2020-10-13 04:47:40</td>\n      <td>17260</td>\n      <td>11469.824091</td>\n      <td>2.808632</td>\n      <td>11469.824091</td>\n      <td>0.017224</td>\n      <td>1035445777</td>\n      <td>11469.824091</td>\n      <td>8.717644e-13</td>\n      <td>1626045</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2020-10-13 04:47:50</td>\n      <td>17270</td>\n      <td>11469.824091</td>\n      <td>2.808632</td>\n      <td>11469.824091</td>\n      <td>0.017224</td>\n      <td>1035445777</td>\n      <td>11469.824091</td>\n      <td>8.717644e-13</td>\n      <td>1626046</td>\n    </tr>\n  </tbody>\n</table>\n<p>1626047 rows Ã— 10 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "top_ob_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "train_depth_dyn, train_dt_index_dyn = reshape_lob_levels(train_dyn_df, output_type='array') # 1 train dataset\n",
    "mid_px_train_dyn = pd.Series((train_depth_dyn[:,2] + train_depth_dyn[:,0]) / 2) # 2\n",
    "px_ts_train = top_ob_train.reset_index()[['Mid_Price']]\n",
    "\n",
    "# test\n",
    "test_depth_dyn, test_dt_index_dyn = reshape_lob_levels(test_dyn_df, output_type='array') # 1 test dataset\n",
    "mid_px_test_dyn = pd.Series((test_depth_dyn[:,2] + test_depth_dyn[:,0]) / 2) # 2\n",
    "px_ts_test = top_ob_test.reset_index()[['Mid_Price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(top_ob_test.index.date).unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_cached_data.shape, top_ob_train.shape"
   ]
  },
  {
   "source": [
    "## Labels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Train Labels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "end = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant to add to avoid negative value (problems with log rets)\n",
    "# const = -min(mid_px_train_dyn.min(), mid_px_test_dyn.min())  + 0.1\n",
    "# mid_px_train_dyn_shifted = mid_px_train_dyn.rename('mid_px_dyn')\n",
    "# mid_px_train_dyn_shifted = mid_px_train_dyn_shifted + const\n",
    "mid_px_train = px_ts_train['Mid_Price']\n",
    "# train labels\n",
    "train_labels_gen = Labels_Generator(mid_px_train)\n",
    "\n",
    "#step 1\n",
    "print('\\n##### Step 1 #####')\n",
    "train_labels_gen.get_raw_labels()\n",
    "label_insights(train_labels_gen.labels)\n",
    "\n",
    "\n",
    "# step 2 - first cleaning\n",
    "print('\\n##### Step 2 #####')\n",
    "df_trades2 = train_labels_gen.get_cleaned_labels(fillna_method='ffill', gross_returns=0.005, trade_len=20)\n",
    "label_insights(train_labels_gen.labels)\n",
    "\n",
    "# step 3 - second cleaning\n",
    "print('\\n##### Step 3 #####')\n",
    "df_trades3 = train_labels_gen.get_cleaned_labels(fillna_value=0, gross_returns=0.005, trade_len=30)#, gross_returns=0.002)\n",
    "label_insights(train_labels_gen.labels)\n",
    "viz_t.plot_labels_line(mid_px_train[start:end], \n",
    "    train_labels_gen.labels[start:end], \n",
    "    title='Train Labels', \n",
    "    smoothed_signal=train_labels_gen.get_smooth_px()[start:end])\n",
    "\n",
    "labels_train = train_labels_gen.labels\n",
    "\n",
    "# get transaction df\n",
    "strategy_df_train = get_strategy_pnl(mid_px_train, labels_train)\n",
    "\n",
    "# encode\n",
    "encoded_train_labels = np_utils.to_categorical(labels_train.values,3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_t.plot_trades_distribution(df_trades3[df_trades3['cleaned_labels']!=0], bin_size=0.0001, metric='gross_returns', fig_width=900, fig_height=550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_t.plot_trades_length_overview(df_trades3[df_trades3['cleaned_labels']!=0], x='trade_len',  y='gross_returns')"
   ]
  },
  {
   "source": [
    "#### Test labels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant to add to avoid negative value (problems with log rets)\n",
    "# mid_px_test_dyn_shifted = mid_px_test_dyn.rename('mid_px_dyn')\n",
    "# mid_px_test_dyn_shifted = mid_px_test_dyn_shifted + const\n",
    "mid_px_test = px_ts_test['Mid_Price']\n",
    "# test labels\n",
    "test_labels_gen = Labels_Generator(mid_px_test)\n",
    "\n",
    "#step 1\n",
    "print('\\n##### Step 1 #####')\n",
    "test_labels_gen.get_raw_labels()\n",
    "label_insights(test_labels_gen.labels)\n",
    "\n",
    "# step 2 - first cleaning\n",
    "print('\\n##### Step 2 #####')\n",
    "df_trades2 = test_labels_gen.get_cleaned_labels(fillna_method='ffill', gross_returns=0.005, trade_len=20)\n",
    "label_insights(test_labels_gen.labels)\n",
    "\n",
    "# step 3 - second cleaning\n",
    "print('\\n##### Step 3 #####')\n",
    "df_trades3 = test_labels_gen.get_cleaned_labels(fillna_value=0, gross_returns=0.005, trade_len=30)#, gross_returns=0.002)\n",
    "label_insights(test_labels_gen.labels)\n",
    "viz_t.plot_labels_line(mid_px_test[start:end], \n",
    "    test_labels_gen.labels[start:end], \n",
    "    title='test Labels', \n",
    "    smoothed_signal=test_labels_gen.get_smooth_px()[start:end])\n",
    "\n",
    "labels_test = test_labels_gen.labels\n",
    "\n",
    "# get transaction df\n",
    "strategy_df_test = get_strategy_pnl(mid_px_test, labels_test)\n",
    "\n",
    "# encode\n",
    "encoded_test_labels = np_utils.to_categorical(labels_test.values,3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trades3[df_trades3['cleaned_labels']!=0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_t.plot_trades_distribution(df_trades3[df_trades3['cleaned_labels']!=0], bin_size=0.0001, metric='gross_returns', fig_width=900, fig_height=550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_t.plot_trades_length_overview(df_trades3[df_trades3['cleaned_labels']!=0], x='trade_len',  y='gross_returns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trades_test = strategy_df_test.dropna(subset=['gross_returns'])\n",
    "trades_test.groupby('labels')['trade_len'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trades_train = strategy_df_train.dropna(subset=['gross_returns'])\n",
    "trades_train.groupby('labels')['trade_len'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram()\n",
    "fig.add_trace(go.Histogram(x=trades_train['trade_len'].values, name='train', autobinx = False, xbins={'size':5}))\n",
    "fig.add_trace(go.Histogram(x=trades_test['trade_len'].values, name='test', autobinx = False, xbins={'size':5}))\n",
    "\n",
    "# The two histograms are drawn on top of another\n",
    "fig.update_layout(barmode='overlay')\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "source": [
    "## Visual check"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLED MID PX CHART - create a func tool function for this\n",
    "sample_size = 6 * 5#6*60*24 # daily\n",
    "dynz_gap = int(roll / sample_size)\n",
    "hourly_mid_line = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "y_train = top_ob_train['Mid_Price'].iloc[::sample_size].values\n",
    "x_train = np.arange(y_train.shape[0])\n",
    "y_test = top_ob_test['Mid_Price'].iloc[::sample_size].values\n",
    "x_test = np.arange(y_train.shape[0] + dynz_gap, y_train.shape[0] + y_test.shape[0] + dynz_gap)\n",
    "\n",
    "y_train_dynz = mid_px_train_dyn.iloc[::sample_size].values  \n",
    "x_train_dynz = np.arange(y_train.shape[0])\n",
    "y_test_dynz = mid_px_test_dyn.iloc[::sample_size].values\n",
    "x_test_dynz = np.arange(y_train.shape[0] + dynz_gap, y_train.shape[0] + y_test.shape[0] + dynz_gap)\n",
    "\n",
    "hourly_mid_line.add_trace(go.Scatter(y=y_train, x=x_train, name='mid_train'), secondary_y=False)\n",
    "hourly_mid_line.add_trace(go.Scatter(y=y_test, x=x_test, name='mid_test'), secondary_y=False)\n",
    "hourly_mid_line.add_trace(go.Scatter(y=y_train_dynz, x=x_train_dynz, name='mid_train_dynz',\n",
    "    marker=dict(color='rgba(44, 130, 201, 0.3)')), secondary_y=True)\n",
    "hourly_mid_line.add_trace(go.Scatter(y=y_test_dynz, x=x_test_dynz, name='mid_test_dynz',\n",
    "    marker=dict(color='rgba(240, 52, 52, 0.3)')), secondary_y=True)\n",
    "\n",
    "hourly_mid_line.update_yaxes(fixedrange= True, secondary_y=True)\n",
    "\n",
    "hourly_mid_line.update_layout(title='<b>Sampled mid</b>')\n",
    "hourly_mid_line.show()"
   ]
  },
  {
   "source": [
    "## Model Training & Settings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_light_deeplob(T, lob_depth):\n",
    "    ## big lr, big batch size 16 filter size, shuffle\n",
    "\n",
    "    input_lmd = Input(shape=(T, lob_depth * 4, 1))\n",
    "    conv_first1 = Conv2D(16, (1, 2), strides=(1, 2))(input_lmd)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    # conv_first1 = Conv2D(32, (1, 20), padding='same')(conv_first1)\n",
    "    # conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    # conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    # conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    \n",
    "    conv_first1 = BatchNormalization()(conv_first1)\n",
    "    # conv_first1 = Dropout(.5)(conv_first1)\n",
    "    \n",
    "    # note on learnable parameters: CONV2(filter shape =1*2, stride=1) layer is: ((shape of width of filter * shape of height filter * number of filters in the previous layer+1) * number of filters) = 2080 or ((2*1*32)+1)*32\n",
    "    conv_first1 = Conv2D(16, (1, 2), strides=(1, 2))(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    # conv_first1 = Dropout(.5)(conv_first1)\n",
    "    # conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    # conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    # conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    # conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = BatchNormalization()(conv_first1)\n",
    "\n",
    "    conv_first1 = Conv2D(16, (1, lob_depth))(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    # conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    # conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    # conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    # conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    \n",
    "    conv_first1 = BatchNormalization()(conv_first1)\n",
    "    print(conv_first1.shape)\n",
    "    # conv_first1 = Dropout(.5)(conv_first1)\n",
    "\n",
    "            \n",
    "    # # build the inception module\n",
    "    # convsecond_1 = Conv2D(32, (1, 1), padding='same')(conv_first1)\n",
    "    # convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "    # convsecond_1 = Conv2D(32, (3, 1), padding='same')(convsecond_1)\n",
    "    # convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "    # convsecond_1 = BatchNormalization()(convsecond_1)\n",
    "    # # convsecond_1 = Dropout(.5)(convsecond_1)\n",
    "\n",
    "    # convsecond_2 = Conv2D(32, (1, 1), padding='same')(conv_first1)\n",
    "    # convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "    # convsecond_2 = Conv2D(32, (5, 1), padding='same')(convsecond_2)\n",
    "    # convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "    \n",
    "    # convsecond_2 = BatchNormalization()(convsecond_2)\n",
    "    # convsecond_2 = Dropout(.5)(convsecond_2)\n",
    "    # convsecond_3 = MaxPooling2D((3, 1), strides=(1, 1), padding='same')(conv_first1)\n",
    "    # convsecond_3 = Conv2D(32, (1, 1), padding='same')(convsecond_3)\n",
    "    # convsecond_3 = LeakyReLU(alpha=0.01)(convsecond_3)\n",
    "    # convsecond_3 = BatchNormalization()(convsecond_3)\n",
    "    # convsecond_3 = Dropout(.5)(convsecond_3)\n",
    "    \n",
    "    # convsecond_output = concatenate([convsecond_1, convsecond_2, convsecond_3], axis=3) #, convsecond_3, convsecond_4\n",
    "    # print(convsecond_output.shape)\n",
    "\n",
    "    # # use the MC dropout here\n",
    "    # conv_reshape = Reshape((int(convsecond_output.shape[1])* int(convsecond_output.shape[3]),))(convsecond_output)\n",
    "    # print(conv_reshape)\n",
    "    convfirst_output = Reshape((int(conv_first1.shape[1])* int(conv_first1.shape[3]),))(conv_first1)\n",
    "    print(convfirst_output.shape)\n",
    "    # note on learnable parameters:FC3 layer is((current layer c*previous layer p)+1*c) with c being number of neurons\n",
    "    out = Dense(3, activation='softmax')(convfirst_output)\n",
    "    print(out.shape)\n",
    "    model = Model(inputs=input_lmd, outputs=out)\n",
    "    adam = Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model_code = inspect.getsource(create_light_deeplob)\n",
    "lines_with_short_desription = [line for line in model_code.split('\\n') if \"##\" in line]\n",
    "short_description = re.sub(r'\\W+', '_', lines_with_short_desription[0])\n",
    "\n",
    "create_light_deeplob(length, lob_depth).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time_now = datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "experiment_id = f'{date_time_now}-{pair}-{frequency_seconds}s-{lob_depth}l-{length}-{date_start}-{date_end}{short_description}'\n",
    "experiment_folder = f'{experiments_folder}/{pair}/{experiment_id}'\n",
    "os.makedirs(f'{experiment_folder}', exist_ok=True)\n",
    "batch_size=256\n",
    "\n",
    "config = {\n",
    "  'pair': pair,\n",
    "  'frequency': frequency_seconds,\n",
    "  'lob_depth': lob_depth,\n",
    "  'length': length,\n",
    "  'date_start': date_start,\n",
    "  'date_end': date_end,\n",
    "  'norm_type': norm_type,\n",
    "  'roll': roll,\n",
    "  'k_plus': k_plus,\n",
    "  'k_minus': k_minus,\n",
    "  'alpha': alpha,\n",
    "  'trading_fee': trading_fee,\n",
    "  'min_profit': min_profit,\n",
    "  'batch_size': batch_size,\n",
    "  'input': input_file_name,\n",
    "  'normalized_train_file': normalized_train_file,\n",
    "  'normalized_test_file':   normalized_test_file,\n",
    "  'top_ob_train_file': top_ob_train_file,\n",
    "  'top_ob_test_file': top_ob_test_file\n",
    "}\n",
    "\n",
    "with open(f'{experiment_folder}/config.json', 'w') as fp:\n",
    "    json.dump(config, fp, default=str)\n",
    "\n",
    "with open(f'{experiment_folder}/model_code.py', 'w') as fp:\n",
    "    fp.write(model_code)\n",
    "\n",
    "light_deeplob = create_light_deeplob(length, lob_depth)\n",
    "with open(f'{experiment_folder}/model_summary.txt', 'w') as fp:\n",
    "    light_deeplob.summary(print_fn=lambda x: fp.write(x + '\\n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to train the model on smoother version of the data"
   ]
  },
  {
   "source": [
    "## Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_deeplob = create_light_deeplob(length, lob_depth)\n",
    "\n",
    "model_checkpoint_path = f'{experiment_folder}/{experiment_id}.h5'\n",
    "\n",
    "# Learning rate callback. Reduce on Plateau multiply the lr by the factor if val loss does not improve for n epochs (patience)\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                                   factor=0.2, \n",
    "                                                   patience=20)\n",
    "\n",
    "# Checkpoint callback. Saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(model_checkpoint_path,\n",
    "                                                 save_best_only=True,\n",
    "                                                 save_weights_only=False,\n",
    "                                                 verbose=2,\n",
    "                                                 save_freq='epoch') # every epoch\n",
    "\n",
    "# Early stopping callback. When sees no progress on the validation set\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(patience=50,\n",
    "                                               restore_best_weights=True)\n",
    "\n",
    "# Tensorboard callback\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(experiment_folder)\n",
    "\n",
    "# Train and Test time series generators\n",
    "generator_train = TimeseriesGenerator(\n",
    "    train_depth_dyn,\n",
    "    encoded_train_labels,\n",
    "    length,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# to be replaced with validation?\n",
    "generator_test = TimeseriesGenerator(\n",
    "    test_depth_dyn,\n",
    "    encoded_test_labels,\n",
    "    length,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "# This may generate warnings related to saving the state of the optimizer.\n",
    "# These warnings (and similar warnings throughout this notebook)\n",
    "# are in place to discourage outdated usage, and can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = '/home/federico/Python_vsc_dir/RL_Trader/Experiments/USDT_BTC/210119-184504-USDT_BTC-10s-10l-300-2020_04_04-2021_01_03_binary_classification_full_inception_lighter_deep_lob_model_with_longer_timesteps_300_/210119-184504-USDT_BTC-10s-10l-300-2020_04_04-2021_01_03_binary_classification_full_inception_lighter_deep_lob_model_with_longer_timesteps_300_.h5'\n",
    "# loaded_light_deep_lob = tf.keras.models.load_model(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "light_deeplob.fit(generator_train, \n",
    "            epochs=200, \n",
    "            verbose=0,\n",
    "            validation_data=generator_test,\n",
    "            callbacks=[lr_callback, cp_callback, es_callback, tb_callback])"
   ]
  },
  {
   "source": [
    "### Model results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '/home/federico/Python_vsc_dir/RL_Trader/Experiments/USDT_BTC/210221-200759-USDT_BTC-10s-10l-100-2020_04_04-2021_01_03_big_lr_big_batch_size_16_filter_size_shuffle/210221-200759-USDT_BTC-10s-10l-100-2020_04_04-2021_01_03_big_lr_big_batch_size_16_filter_size_shuffle.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previously saved weights and evaluate model performance\n",
    "deep_lob_loaded = tf.keras.models.load_model(model_name)\n",
    "generator_test = TimeseriesGenerator(\n",
    "    test_depth_dyn,\n",
    "    encoded_test_labels,\n",
    "    length,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "def evaluate_model(model):\n",
    "    # Re-evaluate the model\n",
    "    loss, acc = model.evaluate(generator_test, verbose=2)\n",
    "    print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate_model(deep_lob_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted labels\n",
    "predictions_prob = deep_lob_loaded.predict(generator_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_prob_wa = pd.DataFrame(predictions_prob).rolling(window=10).mean().values\n",
    "map_labels = np.vectorize(back_to_labels) # vectorize back to labels from func_tools\n",
    "predicted_labels_wa = pd.Series(map_labels(np.argmax(predictions_prob_wa,axis=1)), name='predicted_labels_wa') # back to original 1,0,-1\n",
    "predicted_labels = pd.Series(map_labels(np.argmax(predictions_prob,axis=1)), name='predicted_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('##### Predicted labels #####')\n",
    "label_insights(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('##### Weighted average predicted labels #####')\n",
    "label_insights(predicted_labels_wa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels.shape, test_depth_dyn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dangerous assigning offset here, wrap it into a function\n",
    "offset=100 # offset for plotting\n",
    "start=0\n",
    "end=10000\n",
    "# align prediction offset\n",
    "index_range = np.arange(offset, predicted_labels.shape[0] + offset)\n",
    "predicted_labels.index = index_range\n",
    "buy_prob = pd.Series(predictions_prob[:,1], index=index_range)\n",
    "sell_prob = pd.Series(predictions_prob[:,2], index=index_range)\n",
    "zero_prob = pd.Series(predictions_prob[:,0], index=index_range)\n",
    "\n",
    "buy_prob_wa = pd.Series(predictions_prob_wa[:,1], index=index_range)\n",
    "\n",
    "plot_labels_line(top_ob_test['Mid_Price'][start:end], \n",
    "    test_labels_gen.labels[start:end], \n",
    "    title='Train Labels', \n",
    "    #smoothed_signal=test_labels_gen.get_smooth_px()[start:end],\n",
    "    predicted_labels=predicted_labels[start:end],\n",
    "    buy_prob_labels=buy_prob[start:end],\n",
    "    predictions_prob_wa=buy_prob_wa[start:end],\n",
    "    #sell_prob_labels=sell_prob[start:end],\n",
    "    #dun_px_label=(mid_px_test_dyn_shifted[start:end] - mid_px_test_dyn_shifted.mean())/mid_px_test_dyn_shifted.std()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = go.Figure(data=go.Scatter(x=buy_prob.index, y=buy_prob.values))\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ob_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_ts, vol_ts = intraday_vol_ret(mid, span=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_ts[10000:55000].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_ts[10000:55000].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ob_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_ob_test.index = pd.to_datetime(top_ob_test['Datetime'])\n",
    "\n",
    "# mid = top_ob_test['Mid_Price']\n",
    "# mid = mid[:100000]\n",
    "# smooth_mid = Labels_Generator(mid).get_smooth_px()\n",
    "\n",
    "# smooth_mid.index = output.index\n",
    "# smooth_mid.name = 'Smoothed_mid'\n",
    "\n",
    "# import labelling_class\n",
    "# labelling_class.three_barrier_labelling(smooth_mid, h=700, factor=[1.0020, 0.9980])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_ts = top_ob_test['Mid_Price'][100:].reset_index()['Mid_Price']# adjust prediction offsset\n",
    "datetime_ts = top_ob_test['Datetime'][100:].reset_index()['Datetime']\n",
    "trades_timeseries = get_strategy_pnl(px_ts, predicted_labels)\n",
    "df_trades = trades_timeseries.dropna(subset=['gross_returns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(px_ts, predicted_labels, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_prob = pd.Series(predictions_prob[:,1], name='buy_prob')\n",
    "sell_prob = pd.Series(predictions_prob[:,2], name='sell_prob')\n",
    "zero_prob = pd.Series(predictions_prob[:,0], name='zero_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_ts#top_ob_test[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### to do:\n",
    "# need a sliding window to calculate rolling volatity - not sure about using rolling\n",
    "# seek for patterns in prediction probability\n",
    "# day vs night - weekday vs weekend - model certainty before long trades vs short trades\n",
    "# plot original labels and compare visually (could be part of db)\n",
    "# determine if predictions are naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand with other components of the order book\n",
    "timeseries_results = pd.concat([datetime_ts, trades_timeseries, buy_prob, sell_prob, zero_prob], axis=1)\n",
    "timeseries_results['10min_std'] = timeseries_results['log_ret'].rolling(6*10).std()\n",
    "timeseries_results['1hr_std'] = timeseries_results['log_ret'].rolling(6*60).std()\n",
    "timeseries_results['1d_std'] = timeseries_results['log_ret'].rolling(6*60*24).std()\n",
    "# # np.std(top_ob['log_rets'])\n",
    "# ten_s_std = np.sqrt(np.sum((timeseries_results['log_ret'] - timeseries_results['log_ret'].mean())**2)/(timeseries_results['log_ret'].shape[0]-1)) # -1 unbiased estimator\n",
    "# one_h_std = ten_s_std * np.sqrt(6*60) # assuming statistic independence of returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_results['log_ret'][:300000].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "            x = timeseries_results['Datetime'],\n",
    "            y = timeseries_results['1hr_std']\n",
    "    ),\n",
    "    secondary_y=False\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "            x = timeseries_results['Datetime'],\n",
    "            y = timeseries_results['1d_std']\n",
    "    ),\n",
    "    secondary_y=False\n",
    ")\n",
    "\n",
    "#fig.update_layout(showlegend=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.sum((timeseries_results['log_ret'] - timeseries_results['log_ret'].mean())**2)/(timeseries_results['log_ret'].shape[0]-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trades_timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trades_distribution(df_trades, bin_size=0.0001, metric='gross_returns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trades_length_overview(df_trades, x='trade_len',  y='gross_returns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}